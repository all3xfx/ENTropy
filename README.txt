ENTropy
20 Questions - Characters from literature
by Veronica Lynn and Katherine Siegal
4 June 2012

Our project is a 20-questions game (with a literary characters theme) based on the idea of a decision tree algorithm, but employing a number of other algorithms too. We are turning in the actual program, algorithm.py, which uses db.py to set up the database, along with a database of characters and traits (characters.db). In order to run our program you must first install peewee (download from https://github.com/coleifer/peewee). The program is run by calling algorithm.py from the command line. Think of a character from a book! The program will prompt you with a series of yes or no questions. You may respond with "y" or "yes", "n" or "no", or "u" or "unknown" - none of these is case sensitive. After 20 questions, the program will guess what character you were thinking of, and it will prompt you to confirm or deny whether the guess is correct. If the program's guess was incorrect, you will be asked to input the name of the character you were thinking of and the book they're from.

characters.db keeps track of each character's name, what book they're from, how many times they've been guessed, and their association with each of the questions our program asks. This association, or count, for each question is incremented any time the player answers "yes" for this question while thinking of the given character, and decremented if they answer "no". The count is unchanged if the answer is "unknown".

The overall structure of our program is visible in the run() function, which is called in main(). We will now step through and explain potentially ambiguous portions. 

First, the ask_question() function is called. On the first time through the run() loop, ask_question will select a random question from the database and present it. We had originally coded ask_question() such that on the first iteration it called the function askFirstQuestion(), which chose the first question based on what would split the characters most evenly. However, we switched to the random first question method because we felt our program was getting stuck too easily in the same sequence of questions. askFirstQuestion() is still in the program, and the call to it in ask_question is commented out. On the remaining iterations, ask_question() calls askAlg(), which will make more sense to explain after we've explained answer_question().

answer_question() is called after each question is asked, and this is where we update the liklihood of each character being the answer so far. We do so by keeping track of a weight for each character, based on their numeric association with each attribute/question as recorded in the database. If the answer to the question was yes, we add each character's association with the question to their weight; if the answer was no, we subtract their association with this question from their weight. We then update the global self.likelyCharacter attribute to the character with the largest weight. We also update self.answerPath, which lists the current series of questions that have been asked and the answers that were given to them as tuples.

After the first iteration, askAlg() uses these weights from the previous question and answer to determine the last nineteen questions. On each round it builds a series of decision trees one level deep based on the assumption that our current best guess for self.likelyCharacter is correct. It will choose to ask the question that maximizes the information gain, by asking a question that self.likelyCharacter has a very strong positive or negative association with. That way, if we expect a "yes" to the question we pose next for the character we suspect is the answer and we get a "no", the character we suspected will be heavily penalized. On the other hand, if we get the answer we suspected, we will continue down the same decision tree, making the optimal split at each level.

process-results() is called at the end of run() and updates our database to reflect the game that was just played. If the character the player was thinking of was in the database, it will increment their timesGuessed counter by 1. If this is a new character, it will be added to the database with a timesGuessed value of 1. The function then loops through self.answerPath and, for the character that the player was thinking of, increments their association with each question in the path where the answer was "yes" and decrements the association where the answer was "no".

Notes: Because we only finished our program a couple of days ago and were working on other finals, we had a limited amount of time/manpower to train the program. Ideally, we would have made a website for our algorithm and had lots of people play it, which is our goal in the next couple of weeks. As it stands, the program isn't *great* at guessing who you're thinking of, because it's been trained on a limited amount of people, but it does learn. You might have to guess the same person a couple of times before you see a difference. A couple of characters, unfortunately, received ridiculously high weights in the beginning when we had a bug and couldn't figure out why the program couldn't tell a couple of very different people apart. We played a lot of rounds where we were thinking of these characters. You'll notice the program tends to guess Scout Finch and Galadriel a lot. It does seem to get better and better the more we play it, though, so we don't think we have any major algorithmic problems. Our hope is that if we both continue train it ourselves and are able to get it online where lots of people can train it, it will perform much more accurately.